{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee58285a-5f83-422a-a230-4cf692f1d1fc",
   "metadata": {},
   "source": [
    "Лабораторна  1: Класифікація\n",
    "\n",
    "Мета: Реалізувати програмно алгоритми класифікації 1-Rule, Naive Bayes, Decision Tree, kNN.\n",
    "\n",
    "Тестовий набір:\n",
    "{(0,1,2,1), (1,0,1,0), (0,1,1,1), (0,0,1,1), (0,0,2,1), (1,1,2,1), (1,0,2,0),\r\n",
    "(1,0,0,1), (0,0,0,0), (0,0,1,0)}.Визначити клас для (1,1,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61e257-70af-4083-8163-37963ae7fbf3",
   "metadata": {},
   "source": [
    "Алгоритм 1: 1-Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa1a672-4f0b-424a-94c7-3543560e5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature: f2\n",
      "Rules learned: {1: 1, 0: 0}\n",
      "Prediction for (1, 1, 1): 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "from statistics import mean, stdev\n",
    "\n",
    "data = [\n",
    "    (0,1,2,1), (1,0,1,0), (0,1,1,1), (0,0,1,1), (0,0,2,1),\n",
    "    (1,1,2,1), (1,0,2,0), (1,0,0,1), (0,0,0,0), (0,0,1,0)\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['f1', 'f2', 'f3', 'class'])\n",
    "features = ['f1', 'f2', 'f3']\n",
    "\n",
    "class OneR:\n",
    "    def fit(self, df, target_col):\n",
    "        self.target_col = target_col\n",
    "        self.rules = {}\n",
    "        min_error = float('inf')\n",
    "        best_feature = None\n",
    "\n",
    "        for feature in df.columns:\n",
    "            if feature == target_col:\n",
    "                continue\n",
    "\n",
    "            rule = defaultdict(lambda: None)\n",
    "            total_error = 0\n",
    "\n",
    "            for val in df[feature].unique():\n",
    "                subset = df[df[feature] == val]\n",
    "                most_common = subset[target_col].value_counts().idxmax()\n",
    "                rule[val] = most_common\n",
    "                total_error += sum(subset[target_col] != most_common)\n",
    "\n",
    "            if total_error < min_error:\n",
    "                min_error = total_error\n",
    "                self.rules = rule\n",
    "                self.best_feature = feature\n",
    "\n",
    "    def predict(self, df):\n",
    "        predictions = []\n",
    "        for val in df[self.best_feature]:\n",
    "            pred = self.rules.get(val, None)\n",
    "            predictions.append(pred)\n",
    "        return predictions\n",
    "\n",
    "oner = OneR()\n",
    "oner.fit(df, 'class')\n",
    "\n",
    "# Check which feature was selected and what rules were learned\n",
    "print(\"Best feature:\", oner.best_feature)\n",
    "print(\"Rules learned:\", dict(oner.rules))\n",
    "\n",
    "new_data = pd.DataFrame([(1, 1, 1)], columns=['f1', 'f2', 'f3'])\n",
    "\n",
    "prediction = oner.predict(new_data)\n",
    "print(\"Prediction for (1, 1, 1):\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11086cad-4b2f-47a1-a5a0-08f7a3a7042e",
   "metadata": {},
   "source": [
    "\n",
    "Алгоритм 2: Наївний баєсівський класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e7b6c4-70bd-492a-bd77-26c01084415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for (1,1,1): 1\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.summaries = {}\n",
    "\n",
    "    def fit(self, df):\n",
    "        separated = defaultdict(list)\n",
    "        for _, row in df.iterrows():\n",
    "            separated[row['class']].append(row[:-1])  # exclude class column\n",
    "\n",
    "        summaries = {}\n",
    "        for class_value, rows in separated.items():\n",
    "            feature_stats = []\n",
    "            # For each feature column in class rows, calculate mean and stddev\n",
    "            for col in zip(*rows):\n",
    "                m = mean(col)\n",
    "                s = stdev(col) if len(col) > 1 else 0  # handle single row case\n",
    "                feature_stats.append((m, s))\n",
    "            summaries[class_value] = feature_stats\n",
    "        self.summaries = summaries\n",
    "\n",
    "    def calculate_gaussian_probability(self, x, mean, stdev):\n",
    "        epsilon = 1e-10  # to avoid division by zero\n",
    "        exponent = math.exp(-((x - mean) ** 2) / (2 * (stdev + epsilon) ** 2))\n",
    "        return (1 / (math.sqrt(2 * math.pi) * (stdev + epsilon))) * exponent\n",
    "\n",
    "    def calculate_class_probabilities(self, input_vector):\n",
    "        probabilities = {}\n",
    "        for class_value, class_summaries in self.summaries.items():\n",
    "            probabilities[class_value] = 1\n",
    "            for i in range(len(class_summaries)):\n",
    "                mean, stdev = class_summaries[i]\n",
    "                x = input_vector[i]\n",
    "                probabilities[class_value] *= self.calculate_gaussian_probability(x, mean, stdev)\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        probabilities = self.calculate_class_probabilities(input_vector)\n",
    "        return max(probabilities, key=probabilities.get)\n",
    "\n",
    "gnb = NaiveBayes()\n",
    "gnb.fit(df)\n",
    "\n",
    "new_instance = (1, 1, 1)\n",
    "predicted_class = gnb.predict(new_instance)\n",
    "print(\"Predicted class for (1,1,1):\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858d147-305d-4979-bcd9-29ba13f381ea",
   "metadata": {},
   "source": [
    "Алгоритм 3: Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e1a866-1fcd-49e8-8a21-e16f8e43502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree structure:\n",
      "{'f2': {1: 1, 0: {'f1': {1: {'f3': {1: 0, 2: 0, 0: 1}}, 0: {'f3': {1: 1, 2: 1, 0: 0}}}}}}\n",
      "Prediction for (1, 1, 1): 1\n"
     ]
    }
   ],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "        self.features = None  # Store features names here\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        total = len(labels)\n",
    "        counts = labels.value_counts()\n",
    "        ent = 0\n",
    "        for count in counts:\n",
    "            p = count / total\n",
    "            ent -= p * math.log2(p)\n",
    "        return ent\n",
    "\n",
    "    def info_gain(self, df, feature, target='class'):\n",
    "        total_entropy = self.entropy(df[target])\n",
    "        values = df[feature].unique()\n",
    "        weighted_entropy = 0\n",
    "        for v in values:\n",
    "            subset = df[df[feature] == v]\n",
    "            weighted_entropy += (len(subset) / len(df)) * self.entropy(subset[target])\n",
    "        return total_entropy - weighted_entropy\n",
    "\n",
    "    def majority_class(self, labels):\n",
    "        return labels.value_counts().idxmax()\n",
    "\n",
    "    def build_tree(self, df, features, target='class'):\n",
    "        if len(df[target].unique()) == 1:\n",
    "            return df[target].iloc[0]\n",
    "        if not features:\n",
    "            return self.majority_class(df[target])\n",
    "\n",
    "        gains = {f: self.info_gain(df, f, target) for f in features}\n",
    "        best_feature = max(gains, key=gains.get)\n",
    "\n",
    "        tree = {best_feature: {}}\n",
    "        for v in df[best_feature].unique():\n",
    "            subset = df[df[best_feature] == v]\n",
    "            subtree = self.build_tree(subset, [f for f in features if f != best_feature], target)\n",
    "            tree[best_feature][v] = subtree\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def fit(self, df, features, target='class'):\n",
    "        self.features = features  # Save features list in class\n",
    "        self.tree = self.build_tree(df, features, target)\n",
    "\n",
    "    def predict(self, sample):\n",
    "        tree = self.tree\n",
    "        while isinstance(tree, dict):\n",
    "            feature = next(iter(tree))\n",
    "            if isinstance(sample, dict):\n",
    "                value = sample.get(feature)\n",
    "            else:\n",
    "                # sample is a tuple/list: get index of feature in self.features\n",
    "                feature_index = self.features.index(feature)\n",
    "                value = sample[feature_index]\n",
    "            if value not in tree[feature]:\n",
    "                return None\n",
    "            tree = tree[feature][value]\n",
    "        return tree\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(df, features)\n",
    "\n",
    "test_sample = (1,1,1)\n",
    "prediction = dt.predict(test_sample)\n",
    "\n",
    "print(\"Decision tree structure:\")\n",
    "print(dt.tree)\n",
    "print(f\"Prediction for {test_sample}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f0fbc-edc1-4489-8ccb-b173f7a8dc70",
   "metadata": {},
   "source": [
    "Алгоритм 4: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93972285-4269-4013-81e7-210276716cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for (1, 1, 1): 1\n"
     ]
    }
   ],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        self.features = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    # Fit saves training data\n",
    "    def fit(self, df, features, target='class'):\n",
    "        self.features = features\n",
    "        self.X = df[features].values\n",
    "        self.y = df[target].values\n",
    "\n",
    "    # Euclidean distance between two points\n",
    "    def distance(self, a, b):\n",
    "        return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n",
    "\n",
    "    # Predict class for one sample\n",
    "    def predict_one(self, sample):\n",
    "        # Compute distances to all training points\n",
    "        distances = []\n",
    "        for x_train, label in zip(self.X, self.y):\n",
    "            dist = self.distance(sample, x_train)\n",
    "            distances.append((dist, label))\n",
    "        # Sort by distance\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        # Take k nearest neighbors\n",
    "        k_nearest = distances[:self.k]\n",
    "        # Majority vote\n",
    "        votes = [label for _, label in k_nearest]\n",
    "        most_common = Counter(votes).most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "    # Predict classes for multiple samples\n",
    "    def predict(self, samples):\n",
    "        return [self.predict_one(sample) for sample in samples]\n",
    "\n",
    "knn = KNN(k=3)\n",
    "knn.fit(df, features)\n",
    "\n",
    "test_sample = (1, 1, 1)\n",
    "prediction = knn.predict_one(test_sample)\n",
    "print(f\"Prediction for {test_sample}: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
